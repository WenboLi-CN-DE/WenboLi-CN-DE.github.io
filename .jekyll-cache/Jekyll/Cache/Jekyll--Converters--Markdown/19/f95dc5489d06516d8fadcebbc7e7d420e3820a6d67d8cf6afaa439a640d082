I"¾F<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

<h1 id="neural-networks-ç¥ç»ç½‘ç»œ">Neural Networks ç¥ç»ç½‘ç»œ</h1>

<div style="display: flex;">
    <div style="width: 50%;">

We will learn todayâ€¦

- What a (deep) neural network is
-  How do we train it?
-  â€¦ which requires a calculus refresher â˜º
-  Why is everybody talking about it?
-  Various ways to accelerate gradient descent
-  Practical tips and tricks for training NNs
-  
    </div>

    <div style="width: 50%;">

æˆ‘ä»¬ä»Šå¤©å°†å­¦ä¹ ä»¥ä¸‹å†…å®¹ï¼š

- ä»€ä¹ˆæ˜¯ï¼ˆæ·±åº¦ï¼‰ç¥ç»ç½‘ç»œ
- å¦‚ä½•è®­ç»ƒç¥ç»ç½‘ç»œï¼Ÿ
- è¿™éœ€è¦æ¸©ä¹ ä¸€ä¸‹å¾®ç§¯åˆ† â˜º
- ä¸ºä»€ä¹ˆå¤§å®¶éƒ½åœ¨è°ˆè®ºå®ƒï¼Ÿ
- åŠ é€Ÿæ¢¯åº¦ä¸‹é™çš„å„ç§æ–¹æ³•
- è®­ç»ƒç¥ç»ç½‘ç»œçš„å®é™…æŠ€å·§å’Œè¯€çª

    </div>
</div>

<hr />

<div style="display: flex;">
    <div style="width: 50%;">

Todayâ€˜s Agenda!

-  What is a Neuron?
-  Architectures and Activation Functions
-  Loss-functions
-  Backpropagation and the Chain Rule
-  Computation graphs

Advanced Topics:
- Accelerating gradient descent
- Regularization in Neural Networks
- Practical considerations
  
    </div>
    <div style="width: 50%;">

ä»Šå¤©çš„è®®ç¨‹å¦‚ä¸‹ï¼š

- ä»€ä¹ˆæ˜¯ç¥ç»å…ƒï¼Ÿ
- æ¶æ„å’Œæ¿€æ´»å‡½æ•°
- æŸå¤±å‡½æ•°
- åå‘ä¼ æ’­å’Œé“¾å¼æ³•åˆ™
- è®¡ç®—å›¾

æ·±å…¥è¯é¢˜:
- åŠ é€Ÿæ¢¯åº¦ä¸‹é™
- ç¥ç»ç½‘ç»œä¸­çš„æ­£åˆ™åŒ–
- å®é™…è€ƒè™‘å› ç´ 
  
    </div>
</div>

<hr />

<h2 id="biological-inspiration-the-brain-ç”Ÿç‰©å­¦çµæ„Ÿå¤§è„‘">Biological Inspiration: The brain ç”Ÿç‰©å­¦çµæ„Ÿï¼šå¤§è„‘</h2>

<p><strong>A neuron is the basic computational unit of the brain:</strong></p>

<p><strong>ç¥ç»å…ƒæ˜¯å¤§è„‘çš„åŸºæœ¬è®¡ç®—å•å…ƒï¼š</strong></p>

<p><img src="https://raw.githubusercontent.com/WenboLi-CN-DE/Picture/main/20230614144017.png" alt="" /></p>

<ul>
  <li>Our brain has ~ 1011 neurons</li>
  <li>Each neuron is connected to ~ 104 other neurons (via synapses)</li>
  <li>Synapses have different connectivity</li>
  <li>
    <p>Approx. model: Input impulses are weighted by synapse strength and added up</p>
  </li>
  <li>æˆ‘ä»¬çš„å¤§è„‘æœ‰å¤§çº¦10^11ä¸ªç¥ç»å…ƒã€‚</li>
  <li>æ¯ä¸ªç¥ç»å…ƒé€šè¿‡å¤§çº¦10^4ä¸ªçªè§¦è¿æ¥åˆ°å…¶ä»–ç¥ç»å…ƒã€‚</li>
  <li>çªè§¦å…·æœ‰ä¸åŒçš„è¿æ¥æ–¹å¼ã€‚</li>
  <li>è¿‘ä¼¼æ¨¡å‹ï¼šè¾“å…¥è„‰å†²é€šè¿‡çªè§¦çš„å¼ºåº¦åŠ æƒå¹¶ç›¸åŠ ã€‚</li>
</ul>

<p>Neurons receive input signals and accumulate voltage. After some threshold they will fire spiking responses (highly non-linear response).</p>

<p>ç¥ç»å…ƒæ¥æ”¶è¾“å…¥ä¿¡å·å¹¶ç§¯ç´¯ç”µå‹ã€‚ åœ¨è¾¾åˆ°æŸä¸ªé˜ˆå€¼åï¼Œå®ƒä»¬å°†æ¿€å‘å°–å³°å“åº”ï¼ˆé«˜åº¦éçº¿æ€§å“åº”ï¼‰ã€‚</p>

<p><img src="https://raw.githubusercontent.com/WenboLi-CN-DE/Picture/main/20230614144249.png" alt="" /></p>

<h2 id="artificial-neurons-äººå·¥ç¥ç»å…ƒ">Artificial Neurons äººå·¥ç¥ç»å…ƒ</h2>

<p><strong>For neural nets, we use a much simpler unit (neuron, perceptron):</strong></p>

<p><strong>å¯¹äºç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬ä½¿ç”¨æ›´ç®€å•çš„å•å…ƒï¼ˆç¥ç»å…ƒã€æ„ŸçŸ¥å™¨ï¼‰ï¼š</strong></p>

<p><img src="https://raw.githubusercontent.com/WenboLi-CN-DE/Picture/main/20230614144359.png" alt="" /></p>

<div style="display: flex;">
    <div style="width: 50%;">

3 ingredients:
- Weighting of the input
- Summation
- Non-linear activation function
    </div>
    <div style="width: 50%;">
    
ä¸‰ä¸ªè¦ç´ ï¼š
- è¾“å…¥çš„åŠ æƒ
- æ€»å’Œè®¡ç®—
- éçº¿æ€§æ¿€æ´»å‡½æ•°
    </div>
</div>

<p>Example we already know:</p>
<ul>
  <li>Logistic regression é€»è¾‘å›å½’</li>
</ul>

\[y=\sigma\left(\mathbf{w}^T \mathbf{x}+b\right)\]

<p><img src="https://raw.githubusercontent.com/WenboLi-CN-DE/Picture/main/20230614144739.png" alt="" /></p>

<h3 id="feedforward-neural-networks-å‰é¦ˆç¥ç»ç½‘ç»œ">Feedforward Neural Networks å‰é¦ˆç¥ç»ç½‘ç»œ</h3>

<p>Building a network:</p>
<ul>
  <li>We can connect lots of units
together into a directed acyclic
graph.</li>
  <li>This gives a feed-forward
neural network. Thatâ€™s in
contrast to recurrent neural
networks, which can have
cycles.</li>
  <li>Typically, units are grouped
together into layers.</li>
  <li>Each layer connects N input units to M output units.</li>
  <li>In the simplest case, all input units are connected to all output units. We call this a fully
connected layer.</li>
  <li>Note: the inputs and outputs for a layer are distinct from the inputs and outputs to the network</li>
</ul>

<p>æ„å»ºä¸€ä¸ªç½‘ç»œï¼š</p>
<ul>
  <li>æˆ‘ä»¬å¯ä»¥å°†å¾ˆå¤šå•å…ƒè¿æ¥åœ¨ä¸€èµ·å½¢æˆä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ã€‚</li>
  <li>è¿™å°±å¾—åˆ°äº†ä¸€ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œã€‚ä¸å¾ªç¯ç¥ç»ç½‘ç»œå½¢æˆå¯¹æ¯”ï¼Œåè€…å¯ä»¥æœ‰å¾ªç¯ã€‚</li>
  <li>é€šå¸¸ï¼Œå•å…ƒè¢«åˆ†ç»„æˆå±‚ã€‚</li>
  <li>æ¯ä¸€å±‚å°†Nä¸ªè¾“å…¥å•å…ƒè¿æ¥åˆ°Mä¸ªè¾“å‡ºå•å…ƒã€‚</li>
  <li>åœ¨æœ€ç®€å•çš„æƒ…å†µä¸‹ï¼Œæ‰€æœ‰è¾“å…¥å•å…ƒéƒ½è¿æ¥åˆ°æ‰€æœ‰è¾“å‡ºå•å…ƒã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºå…¨è¿æ¥å±‚ã€‚</li>
  <li>æ³¨æ„ï¼šå±‚çš„è¾“å…¥å’Œè¾“å‡ºä¸ç½‘ç»œçš„è¾“å…¥å’Œè¾“å‡ºæ˜¯ä¸åŒçš„ã€‚</li>
</ul>

<p><img src="https://raw.githubusercontent.com/WenboLi-CN-DE/Picture/main/20230614145012.png" alt="" /></p>

<ul>
  <li>I.e., each layer has a M x N weight matrix W</li>
  <li>Equation in matrix form: $\mathbf{y}=\phi(\mathbf{W} \mathbf{x}+\mathbf{b})$
    <ul>
      <li>Output units are a function of input units</li>
    </ul>
  </li>
  <li>
    <p>Feedforward neural networks are also often called multi-layer perceptrons (MLPs)</p>
  </li>
  <li>æ¯ä¸€å±‚å…·æœ‰ä¸€ä¸ªå¤§å°ä¸ºM x Nçš„æƒé‡çŸ©é˜µWã€‚</li>
  <li>ä»¥çŸ©é˜µå½¢å¼çš„æ–¹ç¨‹ä¸ºï¼š$\mathbf{y}=\phi(\mathbf{W} \mathbf{x}+\mathbf{b})$
    <ul>
      <li>è¾“å‡ºå•å…ƒæ˜¯è¾“å…¥å•å…ƒçš„å‡½æ•°ã€‚</li>
    </ul>
  </li>
  <li>å‰é¦ˆç¥ç»ç½‘ç»œé€šå¸¸ä¹Ÿè¢«ç§°ä¸ºå¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰</li>
</ul>

<p><img src="https://raw.githubusercontent.com/WenboLi-CN-DE/Picture/main/20230614145355.png" alt="" /></p>

<h3 id="activation-funcitons-æ¿€æ´»å‡½æ•°">Activation funcitons æ¿€æ´»å‡½æ•°</h3>

<p>Different activation functions for introducing non-linearities:
å¼•å…¥éçº¿æ€§çš„ä¸åŒæ¿€æ´»å‡½æ•°ï¼š</p>

<p><img src="https://raw.githubusercontent.com/WenboLi-CN-DE/Picture/main/20230614145512.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/WenboLi-CN-DE/Picture/main/20230614145629.png" alt="" /></p>

<p>è®¡ç®—ï¼š</p>

\[\sigma(x)=\frac{1}{1+\exp (-x)}\]

<ul>
  <li>å°†æ•°å­—å‹ç¼©åˆ°èŒƒå›´[0,1]</li>
  <li>ä»å†å²ä¸Šæ¥çœ‹å®ƒä»¬éå¸¸æµè¡Œï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥å¾ˆå¥½åœ°è§£é‡Šä¸ºç¥ç»å…ƒçš„é¥±å’Œâ€œå‘å°„ç‡â€
    <blockquote>
      <p>å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ï¼ˆå¦‚Sigmoidå‡½æ•°ï¼‰åœ¨è¾“å…¥å€¼è¾ƒå¤§æˆ–è¾ƒå°çš„æƒ…å†µä¸‹ä¼šé¥±å’Œï¼Œå³è¾“å‡ºå€¼æ¥è¿‘0æˆ–1ï¼Œå¹¶å…·æœ‰ç±»ä¼¼äºç¥ç»å…ƒå‘å°„çš„ç‰¹æ€§ã€‚å› æ­¤ï¼Œè¿™äº›æ¿€æ´»å‡½æ•°çš„è¾“å‡ºå€¼å¯ä»¥è¢«è§£é‡Šä¸ºç¥ç»å…ƒçš„é¥±å’Œâ€œå‘å°„ç‡â€ã€‚</p>
    </blockquote>
  </li>
</ul>

<p><strong>é—®é¢˜ï¼š</strong></p>
<ul>
  <li>é¥±å’Œçš„ç¥ç»å…ƒä¼šä½¿å¾—æ¢¯åº¦æ¶ˆå¤±</li>
  <li>Sigmoidå‡½æ•°çš„è¾“å‡ºä¸ä»¥é›¶ä¸ºä¸­å¿ƒï¼ˆå¯¹äºåˆå§‹åŒ–å¾ˆé‡è¦ï¼‰</li>
  <li>exp()è®¡ç®—è€—è´¹èµ„æº</li>
</ul>

<p><img src="https://raw.githubusercontent.com/WenboLi-CN-DE/Picture/main/20230614150009.png" alt="" /></p>

<ul>
  <li>å°†æ•°å­—å‹ç¼©åˆ°èŒƒå›´[-1,1]<br />
âœ“ ä»¥é›¶ä¸ºä¸­å¿ƒï¼ˆå¾ˆå¥½ï¼‰<br />
Ã— å½“é¥±å’Œæ—¶ä»ç„¶ä¼šæ¶ˆå¤±æ¢¯åº¦</li>
</ul>

<p><img src="https://raw.githubusercontent.com/WenboLi-CN-DE/Picture/main/20230614150446.png" alt="" /></p>

<p>ä¿®æ­£çº¿æ€§å•å…ƒï¼ˆRectified Linear Unitï¼ŒReLUï¼‰</p>

<p>è®¡ç®—ï¼š $f(x)=\max (0, x)$</p>

<p>âœ“ ä¸ä¼šé¥±å’Œï¼ˆåœ¨æ­£åŒºé—´å†…ï¼‰<br />
âœ“ è®¡ç®—æ•ˆç‡éå¸¸é«˜<br />
âœ“ åœ¨å®è·µä¸­æ¯”sigmoid/tanhå‡½æ•°æ”¶æ•›é€Ÿåº¦å¿«å¾—å¤šï¼ˆä¾‹å¦‚ï¼Œå¿«6å€ï¼‰</p>

<p>Ã— è¾“å‡ºä¸ä»¥é›¶ä¸ºä¸­å¿ƒ<br />
Ã— å¯¹äºx &lt; 0æ²¡æœ‰æ¢¯åº¦</p>

<p><img src="https://raw.githubusercontent.com/WenboLi-CN-DE/Picture/main/20230614152248.png" alt="" /></p>

<p>è®¡ç®—ï¼š $f(x)=\max (0.1 x, x)$</p>

<p>âœ“ ä¸ä¼šé¥±å’Œ<br />
âœ“ è®¡ç®—æ•ˆç‡é«˜<br />
âœ“ åœ¨å®è·µä¸­æ”¶æ•›é€Ÿåº¦æ¯”sigmoid/tanhå‡½æ•°å¿«å¾ˆå¤šï¼ï¼ˆä¾‹å¦‚ï¼Œ6å€ï¼‰<br />
âœ“ ä¸ä¼šâ€œæ¶ˆå¤±â€</p>

<p>Parametric Rectifier (PReLu):<br />
å‚æ•°æ•´æµå™¨ (PReLu)ï¼š</p>

<p>è®¡ç®—ï¼š $f(x)=\max (\alpha x, x)$</p>

<p>Also learn alpha</p>

<p><img src="https://raw.githubusercontent.com/WenboLi-CN-DE/Picture/main/20230614152454.png" alt="" /></p>

<p>æŒ‡æ•°çº¿æ€§å•å…ƒ</p>

<p>è®¡ç®—ï¼š</p>

\[f(x)= \begin{cases}x &amp; \text { if } x&gt;0 \\ \alpha(\exp (x)-1) &amp; \text { if } x \leq 0\end{cases}\]

<p>å…¶ä¸­ï¼Œalphaæ˜¯ä¸€ä¸ªé¢„å®šä¹‰çš„å¸¸æ•°ï¼Œé€šå¸¸å–ä¸€ä¸ªè¾ƒå°çš„æ­£æ•°ã€‚</p>

<p>âœ“ å…·æœ‰ReLUçš„æ‰€æœ‰ä¼˜ç‚¹<br />
âœ“ è¾“å‡ºæ¥è¿‘é›¶å‡å€¼<br />
Ã— è®¡ç®—è¿‡ç¨‹ä¸­éœ€è¦ä½¿ç”¨exp()å‡½æ•°</p>

<p><strong>In practice:</strong></p>

<ul>
  <li>ä½¿ç”¨ReLUã€‚åœ¨å­¦ä¹ ç‡å’Œåˆå§‹åŒ–æ—¶è¦å°å¿ƒã€‚
    <blockquote>
      <p>å¯¹äºå­¦ä¹ ç‡ï¼ˆlearning rateï¼‰ï¼Œé€‰æ‹©ä¸€ä¸ªåˆé€‚çš„å€¼éå¸¸é‡è¦ã€‚è¿‡å¤§çš„å­¦ä¹ ç‡å¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®šæˆ–å‘æ•£ï¼Œè€Œè¿‡å°çš„å­¦ä¹ ç‡å¯èƒ½å¯¼è‡´æ”¶æ•›é€Ÿåº¦è¿‡æ…¢ã€‚</p>

      <p>å¯¹äºåˆå§‹åŒ–ï¼ˆinitializationï¼‰ï¼Œæƒé‡å’Œåç½®çš„åˆå§‹å€¼ä¹Ÿéœ€è¦è°¨æ…é€‰æ‹©ã€‚ä½¿ç”¨ä¸åˆé€‚çš„åˆå§‹åŒ–æ–¹æ³•å¯èƒ½å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±æˆ–æ¢¯åº¦çˆ†ç‚¸ç­‰é—®é¢˜ï¼Œå½±å“ç½‘ç»œçš„è®­ç»ƒæ•ˆæœã€‚å¯¹äºä½¿ç”¨ReLUçš„ç½‘ç»œï¼Œä¸€ç§å¸¸è§çš„åˆå§‹åŒ–æ–¹æ³•æ˜¯ä½¿ç”¨è¾ƒå°çš„éšæœºå€¼ï¼Œå¦‚ä»å‡åŒ€åˆ†å¸ƒæˆ–æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·å¾—åˆ°çš„å€¼ã€‚</p>
    </blockquote>
  </li>
  <li>å°è¯•ä½¿ç”¨Leaky ReLU / ELUã€‚</li>
  <li>å°è¯•ä½¿ç”¨tanhå‡½æ•°ï¼Œä½†ä¸è¦æœŸæœ›å¤ªå¤šã€‚</li>
  <li>ä¸è¦ä½¿ç”¨sigmoidå‡½æ•°ã€‚
    <ul>
      <li>sigmoidå‡½æ•°ä»…åœ¨åˆ†ç±»é—®é¢˜çš„è¾“å‡ºæ¿€æ´»ä¸­ä½¿ç”¨ã€‚</li>
    </ul>
  </li>
</ul>

<p>Formalisation:</p>

<p>æ¯å±‚è®¡ç®—ä¸€ä¸ªå‡½æ•°ï¼Œå› æ­¤ç½‘ç»œè®¡ç®—å‡½æ•°çš„ç»„åˆï¼š</p>

\[\begin{aligned}
\mathbf{h}^{(1)} &amp; =f^{(1)}(\mathbf{x}) \\
\mathbf{h}^{(2)} &amp; =f^{(2)}\left(\mathbf{h}^{(1)}\right) \\
\vdots &amp;
\end{aligned}\]

<p>æˆ–è€…æ›´ç®€å•åœ°ï¼š</p>

\[\begin{aligned}
&amp; \mathbf{y}=f^{(L)}\left(\mathbf{h}^{(L-1)}\right) \\
&amp; \mathbf{y}=f^L \circ f^{L-1} \circ \ldots f^{(1)}(\mathbf{x})
\end{aligned}\]

<p>ç¥ç»ç½‘ç»œæä¾›æ¨¡å—åŒ–ï¼šæˆ‘ä»¬å¯ä»¥å°†æ¯ä¸€å±‚çš„è®¡ç®—å®ç°ä¸ºä¸€ä¸ªé»‘ç›’å­</p>

<p><img src="https://raw.githubusercontent.com/WenboLi-CN-DE/Picture/main/20230614154844.png" alt="" /></p>

<h4 id="example-xor-å¼‚æˆ–">Example: XOR å¼‚æˆ–</h4>

<p>è®¾è®¡ä¸€ä¸ªå®ç° XOR çš„ç½‘ç»œï¼š</p>

<p><img src="https://raw.githubusercontent.com/WenboLi-CN-DE/Picture/main/20230614161253.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/WenboLi-CN-DE/Picture/main/20230614161308.png" alt="" /></p>

<ul>
  <li>å•ä¸ªå•å…ƒæ— æ³•è®¡ç®—!</li>
  <li>ç»å…¸çš„ä¾‹å­ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å¤šå±‚æ¬¡</li>
</ul>

<p><strong>XOR in terms of elemental operations:</strong></p>

<p>XOR(a,b) = (a OR b) AND NOT (a AND b)</p>

<p>è®¾è®¡ä¸€ä¸ªå®ç°XORçš„ç½‘ç»œï¼š</p>
<ul>
  <li>æ¿€æ´»å‡½æ•°çš„ç¡¬é˜ˆå€¼ï¼Œx1å’Œx2æ˜¯äºŒè¿›åˆ¶çš„</li>
  <li>h1 è®¡ç®— x1 OR x2</li>
  <li>h2 è®¡ç®— x1 AND x2</li>
  <li>y è®¡ç®— h1 AND NOT h2</li>
</ul>

<p><img src="https://raw.githubusercontent.com/WenboLi-CN-DE/Picture/main/20230614161739.png" alt="" /></p>

<h3 id="deep-architecturesæ·±å±‚æ¶æ„">Deep Architecturesæ·±å±‚æ¶æ„</h3>

<p>ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦æ·±å…¥ï¼Ÿ</p>
<ul>
  <li>ä»»ä½•çº¿æ€§å±‚åºåˆ—éƒ½å¯ä»¥ç”¨å•ä¸ªçº¿æ€§å±‚ç­‰æ•ˆåœ°è¡¨ç¤º</li>
</ul>

\[\mathbf{y}=\underbrace{\mathbf{W}^{(3)} \mathbf{W}^{(2)} \mathbf{W}^{(1)}}_{\tilde{\mathbf{W}}} \mathbf{x}\]

<p>å³ï¼Œæˆ‘ä»¬éœ€è¦éçº¿æ€§ï¼Œä»¥åˆ©ç”¨å¤šä¸ªå±‚æ¬¡</p>

<ul>
  <li>å…·æœ‰éçº¿æ€§æ¿€æ´»å‡½æ•°çš„FF-NNæ˜¯é€šç”¨å‡½æ•°è¿‘ä¼¼å™¨ï¼š
    <ul>
      <li>ç»™å®šä¸€ä¸ªæ½œåœ¨çš„æ— é™é‡çš„å•å…ƒï¼Œå®ƒä»¬å¯ä»¥ä»»æ„åœ°é€¼è¿‘ä»»ä½•å‡½æ•°</li>
      <li>é€šç”¨å‡½æ•°é€¼è¿‘å®šç†ï¼š å•å±‚å°±è¶³ä»¥å®ç° â€œæ™®é€‚æ€§â€</li>
    </ul>
  </li>
</ul>

<p><strong>é‚£ä¹ˆï¼Œå•å±‚æ˜¯å¦è¶³å¤Ÿï¼Ÿ</strong></p>
<ul>
  <li>å°½ç®¡é€šç”¨å‡½æ•°é€¼è¿‘å®šç†è¡¨ç¤ºå•å±‚ç†è®ºä¸Šè¶³å¤Ÿï¼Œä½†å®é™…ä¸Šæˆ‘ä»¬éœ€è¦æŒ‡æ•°çº§ï¼ˆä¸è¾“å…¥ç»´åº¦æˆæ­£æ¯”ï¼‰çš„ç¥ç»å…ƒæ•°é‡æ‰èƒ½å®ç°è¿™ä¸€ç‚¹ã€‚
    <ul>
      <li>å¦‚æœå¯ä»¥å­¦ä¹ ä»»ä½•å‡½æ•°ï¼Œé‚£ä¹ˆç»“æœå¾ˆå¯èƒ½ä¼šè¿‡æ‹Ÿåˆã€‚</li>
    </ul>
  </li>
  <li>ç›¸åï¼Œå¤šå±‚ç½‘ç»œå¯ä»¥ç”¨æ›´å°‘çš„ç¥ç»å…ƒå®ç°ç±»ä¼¼çš„æ•ˆæœã€‚
    <ul>
      <li>ç´§å‡‘çš„è¡¨ç¤ºæ–¹å¼æ¯”â€é€šç”¨è¡¨ç¤ºâ€æ›´æœ‰æ•ˆã€‚</li>
    </ul>
  </li>
</ul>

<h2 id="loss-functions-æŸå¤±å‡½æ•°">Loss-functions æŸå¤±å‡½æ•°</h2>

<p>è®­ç»ƒç¥ç»ç½‘ç»œçš„ç›®æ ‡å‡½æ•°ï¼š</p>

<p>é€šç”¨çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼šé€æ ·æœ¬æŸå¤± + æ­£åˆ™åŒ–æƒ©ç½š</p>

\[\boldsymbol{\theta}^*=\underset{\text { parameters } \boldsymbol{\theta}}{\arg \min } \sum_{i=1}^N l\left(\boldsymbol{x}_i, \boldsymbol{\theta}\right)+\lambda \text { penalty }(\boldsymbol{\theta})\]

<p>å¯¹äºä¸åŒçš„ä»»åŠ¡ï¼ŒæŸå¤±å‡½æ•°å’Œè¾“å‡ºæ¿€æ´»å‡½æ•°çš„é€‰æ‹©æœ‰æ‰€ä¸åŒï¼š</p>
<ul>
  <li>å›å½’ä»»åŠ¡ï¼ˆRegressionï¼‰ï¼šé€šå¸¸ä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMean Squared Errorï¼‰ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œè¾“å‡ºæ¿€æ´»å‡½æ•°å¯ä»¥æ˜¯çº¿æ€§å‡½æ•°æˆ–æ’ç­‰å‡½æ•°ã€‚</li>
  <li>äºŒåˆ†ç±»ä»»åŠ¡ï¼ˆBinary Classificationï¼‰ï¼šå¸¸è§çš„æŸå¤±å‡½æ•°åŒ…æ‹¬äºŒå…ƒäº¤å‰ç†µï¼ˆBinary Cross-Entropyï¼‰æˆ–å¯¹æ•°æŸå¤±ï¼ˆLog Lossï¼‰ï¼Œè¾“å‡ºæ¿€æ´»å‡½æ•°é€šå¸¸é€‰æ‹©sigmoidå‡½æ•°ã€‚</li>
  <li>å¤šç±»åˆ«åˆ†ç±»ä»»åŠ¡ï¼ˆMulti-class Classificationï¼‰ï¼šå¸¸ç”¨çš„æŸå¤±å‡½æ•°æ˜¯å¤šç±»åˆ«äº¤å‰ç†µï¼ˆCategorical Cross-Entropyï¼‰ï¼Œè¾“å‡ºæ¿€æ´»å‡½æ•°åˆ™é€šå¸¸é€‰æ‹©softmaxå‡½æ•°ã€‚</li>
</ul>

<p><strong>Regression å›å½’</strong></p>

<div style="display: flex;">
<div style="flex: 50%;">

<b>Output layer: Deterministic å†³å®šæ€§</b>

linear çº¿æ€§

$$
\mathbf{f}=\mathbf{W}^{(L)} \mathbf{h}^{(L-1)}+\boldsymbol{b}^{(L)}
$$

<b>Lossï¼š</b>

squared error æ–¹å·®

$$
l_i\left(\mathbf{x}_i, \boldsymbol{\theta}\right)=\frac{1}{2}\left(\mathbf{f}\left(\mathbf{x}_i\right)-\mathbf{y}_i\right)^2
$$

</div>
<div style="flex: 50%;">

<b>Probabilistic æ¦‚ç‡æ€§</b>

linear Gaussian

$$
p(\mathbf{y} \mid \mathbf{x})=\mathcal{N}\left(\mathbf{y} \mid \mathbf{W}^{(L)} \mathbf{h}^{(L-1)}+\mathbf{b}^{(L)}, \mathbf{\Sigma}\right)
$$

<br />

negative log-likelihood è´Ÿå¯¹æ•°ä¼¼ç„¶

$$
l_i\left(\mathbf{x}_i, \boldsymbol{\theta}\right)=-\log \mathcal{N}\left(\mathbf{y}_i \mid \boldsymbol{\mu}\left(\mathbf{x}_i\right), \boldsymbol{\Sigma}\right)
$$

</div>
</div>

<p><strong>Binary classification äºŒå…ƒåˆ†ç±»</strong></p>

<div style="display: flex;">
<div style="flex: 50%;">

<b>Output layer: Deterministic å†³å®šæ€§</b>

linear çº¿æ€§

$$
f=\mathbf{W}^{(L)} \mathbf{h}^{(L-1)}+b^{(L)}
$$

<b>Loss function</b>

hinge-loss é“°é“¾æŸå¤±

$$
l\left(\mathbf{x}_i, \boldsymbol{\theta}\right)=\max \left(0,1-y_i f\left(\boldsymbol{x}_i\right)\right)
$$

</div>
<div style="flex: 50%;">

<b>Probabilistic æ¦‚ç‡æ€§</b>

sigmoid 

$$
f=\sigma\left(\mathbf{W}^{(L)} \mathbf{h}^{(L-1)}+b^{(L)}\right)
$$

<br />

neg-loglike è´Ÿå¯¹æ•°ä¼¼ç„¶

$$
\begin{aligned}
l_i\left(\mathbf{x}_i, \boldsymbol{\theta}\right)= &amp; -c_i \log f\left(\mathbf{x}_i\right)-\left(1-c_i\right) \log \left(1-f\left(\mathbf{x}_i\right)\right)
\end{aligned}
$$

</div>
</div>

<p>å…¶ä¸­$y_i$æ˜¯ -1/+1 labels, $c_i$ æ˜¯0/1 labelsã€‚</p>

<p><strong>Multi-class classification å¤šç±»åˆ«åˆ†ç±»</strong></p>

<div style="display: flex;">
<div style="flex: 50%;">

<b>Output layer: Deterministic å†³å®šæ€§</b>

linear çº¿æ€§

$$
\mathbf{f}=\mathbf{W}^{(L)} \mathbf{h}^{(L-1)}+\mathbf{b}^{(L)}
$$

<b>Loss function</b>

Multi-class SVM loss å¤šç±» SVM æŸå¤±<br />

<div style="text-align: center;">
 Not covered
</div>

</div>
<div style="flex: 50%;">

<b>Probabilistic æ¦‚ç‡æ€§</b>

sigmoid 

$$
\mathbf{f}=\operatorname{softmax}\left(\mathbf{W}^{(L)} \mathbf{h}^{(L-1)}+\mathbf{b}^{(L)}\right)
$$

<br />

neg-loglike è´Ÿå¯¹æ•°ä¼¼ç„¶

$$
l_i\left(\mathbf{x}_i, \boldsymbol{\theta}\right)=-\sum_{k=1}^K \boldsymbol{h}_{c_i, k} \log y_k\left(\mathbf{x}_i\right)
$$

</div>
</div>

<p>å…¶ä¸­ $\boldsymbol{h}_{c_i, k}$ æ˜¯ one hot coding</p>
<blockquote>
  <p>One-hot encodingæ˜¯ä¸€ç§å¸¸ç”¨çš„æ•°æ®é¢„å¤„ç†æŠ€æœ¯ï¼Œç”¨äºå°†ç¦»æ•£ç‰¹å¾è¡¨ç¤ºä¸ºäºŒè¿›åˆ¶å‘é‡çš„å½¢å¼ã€‚å®ƒå¸¸ç”¨äºæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ ä»»åŠ¡ä¸­ï¼Œç‰¹åˆ«æ˜¯å½“ç‰¹å¾æ•°æ®ä¸­åŒ…å«åˆ†ç±»å˜é‡æ—¶ã€‚</p>

  <p>åœ¨One-hot encodingä¸­ï¼Œå¦‚æœä¸€ä¸ªç‰¹å¾å…·æœ‰nä¸ªä¸åŒçš„ç±»åˆ«ï¼Œé‚£ä¹ˆå®ƒå°†è¢«è¡¨ç¤ºä¸ºä¸€ä¸ªé•¿åº¦ä¸ºnçš„äºŒè¿›åˆ¶å‘é‡ï¼Œå…¶ä¸­åªæœ‰ä¸€ä¸ªä½ç½®ä¸º1ï¼Œå…¶ä»–ä½ç½®éƒ½ä¸º0ã€‚è¢«è®¾ç½®ä¸º1çš„ä½ç½®å¯¹åº”äºè¯¥ç‰¹å¾æ‰€å±çš„ç±»åˆ«ã€‚</p>

  <p>è¿™æ ·çš„ç¼–ç æ–¹å¼æœ‰åŠ©äºè§£å†³ä»¥ä¸‹é—®é¢˜ï¼š</p>

  <p>è§£å†³åˆ†ç±»å˜é‡çš„æ•°å€¼åŒ–é—®é¢˜ï¼šåˆ†ç±»å˜é‡é€šå¸¸æ— æ³•ç›´æ¥ç”¨äºæœºå™¨å­¦ä¹ ç®—æ³•ï¼Œå› ä¸ºç®—æ³•é€šå¸¸æœŸæœ›è¾“å…¥æ˜¯æ•°å€¼å‹æ•°æ®ã€‚One-hot encodingå¯ä»¥å°†åˆ†ç±»å˜é‡è½¬æ¢ä¸ºæ•°å€¼å‹æ•°æ®ï¼Œä½¿å…¶é€‚ç”¨äºç®—æ³•çš„å¤„ç†ã€‚</p>

  <p>é¿å…ç‰¹å¾ä¹‹é—´çš„é¡ºåºå…³ç³»ï¼šOne-hot encodingå°†æ¯ä¸ªç±»åˆ«éƒ½ç‹¬ç«‹åœ°è¡¨ç¤ºä¸ºä¸€ä¸ªäºŒè¿›åˆ¶å‘é‡ï¼Œä¸è€ƒè™‘ç±»åˆ«ä¹‹é—´çš„é¡ºåºå…³ç³»ã€‚è¿™åœ¨ä¸€äº›æƒ…å†µä¸‹æ˜¯æœ‰ç›Šçš„ï¼Œä¾‹å¦‚é¿å…ç®—æ³•é”™è¯¯åœ°å­¦ä¹ åˆ°ç±»åˆ«ä¹‹é—´çš„é¡ºåºæˆ–å¤§å°å…³ç³»ã€‚</p>

  <p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå½“åŸå§‹ç‰¹å¾å…·æœ‰å¤§é‡ç±»åˆ«æ—¶ï¼ŒOne-hot encodingä¼šå¯¼è‡´ç‰¹å¾ç©ºé—´çš„ç»´åº¦å¢åŠ ï¼Œå¯èƒ½ä¼šå¯¼è‡´ç¨€ç–çŸ©é˜µå’Œè®¡ç®—èµ„æºçš„æµªè´¹ã€‚åœ¨å¤„ç†é«˜ç»´ç¨€ç–æ•°æ®æ—¶ï¼Œå¯èƒ½éœ€è¦è€ƒè™‘å…¶ä»–çš„ç‰¹å¾ç¼–ç æ–¹æ³•ã€‚</p>

  <p>åœ¨å®è·µä¸­ï¼Œå¯ä»¥ä½¿ç”¨å¤šç§ç¼–ç¨‹è¯­è¨€å’Œåº“æ¥æ‰§è¡ŒOne-hot encodingï¼Œä¾‹å¦‚Pythonä¸­çš„scikit-learnã€pandaså’ŒTensorFlowç­‰ã€‚</p>
</blockquote>

<p><strong>Feature Learning ç‰¹å¾å­¦ä¹ </strong></p>

<p>ç¥ç»ç½‘ç»œå¯ä»¥è¢«çœ‹ä½œæ˜¯ä¸€ç§å­¦ä¹ ç‰¹å¾çš„æ–¹å¼</p>
<ul>
  <li>æœ€åä¸€å±‚æ˜¯æ ‡å‡†çš„çº¿æ€§å›å½’/åˆ†ç±»å±‚</li>
</ul>

<p>ç½‘ç»œå­¦ä¹ ç‰¹å¾$\psi(\mathbf{x})$ä½¿å¾—çº¿æ€§å›å½’/åˆ†ç±»å¯ä»¥è§£å†³å®ƒ</p>

<p><img src="https://raw.githubusercontent.com/WenboLi-CN-DE/Picture/main/20230614165612.png" alt="" /></p>

<p><img src="https://raw.githubusercontent.com/WenboLi-CN-DE/Picture/main/20230614165623.png" alt="" /></p>
:ET